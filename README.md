# XOR-mlp-perceptron-python

# XOR Gate using Multi-Layer Perceptron (MLP)

This project implements a **2-layer neural network (Multi-Layer Perceptron)** using **Python and NumPy** to solve the **XOR logic gate problem**.

The XOR problem cannot be solved using a single-layer perceptron. Hence, a hidden layer is added to enable non-linear decision boundaries.

---

## ğŸ“Œ Project Objective
- To implement a perceptron learning algorithm for the XOR gate
- To understand forward propagation and backpropagation
- To demonstrate how a multi-layer perceptron solves a non-linear problem

---

## ğŸ§  Model Architecture
- **Input Layer:** 2 neurons  
- **Hidden Layer:** 2 neurons (Sigmoid activation)  
- **Output Layer:** 1 neuron (Sigmoid activation)

---

## ğŸ§ª Dataset (XOR Truth Table)
| Input 1 | Input 2 | Output |
|--------|--------|--------|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

---

## âš™ï¸ Technologies Used
- Python
- NumPy
- Jupyter Notebook

---

## ğŸš€ How to Run

1. Clone the repository
   ```bash
   git clone https://github.com/aliasyeda/XOR-mlp-perceptron-python.git
Open the Jupyter Notebook

Run all cells sequentially

---

âœ… Output


The model learns the XOR logic using backpropagation

Final predictions match the XOR truth table

Training loss decreases over epochs

---

ğŸ“Œ Conclusion


This project demonstrates how a multi-layer perceptron can successfully learn the XOR function by using a hidden layer and non-linear activation functions, proving the limitation of single-layer perceptrons.

---

âœ¨ Author

Syeda Alia Samia
